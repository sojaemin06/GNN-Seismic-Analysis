# 개발 계획 (Development Plan)

## Phase 1: 데이터셋 구축 (완료)
- [x] OpenSeesPy 기반 구조 해석 엔진 구축 (`src/core/`)
- [x] 데이터 생성 스크립트 및 검증 로직 구현 (`scripts/generate_dataset.py`)
- [x] 그래프 데이터 변환 로직 구현 (`src/data/`)

## Phase 2: 데이터 생성 및 전처리
- [x] **생성 스크립트 고도화**:
    - [x] "All-or-Nothing" 저장 로직 구현 (부분 저장 방지).
    - [x] 기존 샘플 스킵(Resume) 기능 및 상세 에러 로깅 추가.
- [ ] **데이터셋 점진적 확장 (Incremental Expansion)**:
    - **전략**: 기존 유효 데이터를 **삭제하지 않고 보존**하며, 새로운 데이터를 계속 추가하여 누적시킴.
    - **목표 수량**: 고정된 수치가 아닌, `run_data_scalability_experiment.py`를 통한 실험 결과 **데이터 증가 대비 성능 향상(R2) 효율이 포화(Plateau)되는 시점**까지 확장.
    - **실행**: 로컬 또는 서버에서 `generate_dataset.py`를 지속적으로 실행하여 데이터 확보.
- [x] **데이터 정제 및 검증**:
    - [x] 물리적 정합성 검증 (130% 룰, 중력 해석 등).
    - [x] 실패/불완전 샘플 자동 정리 스크립트(`cleanup_and_reindex_data.py`) 구현 완료.
- [ ] **피처 엔지니어링**:
    - [x] 입력(Node/Edge) 및 출력(Curve) 피처 정규화.
    - [ ] (필요시) 모델 성능 한계 도달 시 파생 변수 추가 검토.

## Phase 3: 성능 고도화 및 평가
- **목표:** SCI 논문 수준의 예측 정확도 달성 및 GNN 기반 성능평가(CSM)의 신뢰성 확보.
- **주요 마일스톤:**
    1. **대규모 고품질 데이터셋 구축:**
       - 기존 데이터 전량 폐기 후 500개 샘플 재생성.
       - 엄격한 품질 검증(Flat Curve, Low Strength 제거) 및 정확한 생성 시간 측정 적용.
    2. **GNN 모델 최적화:**
       - 데이터 수에 따른 성능(R2) 및 비용(Time) 분석 실험 재수행.
       - CSM 성능점($S_d$) 오차(현재 >100%) 원인 분석 및 개선 (Loss Function 튜닝 등 고려).
    3. **역량스펙트럼법(CSM) 연동 검증:**
       - 실제 해석 vs GNN 예측 기반 성능점 비교 오차율 10~20% 이내 달성 목표.

## Phase 4: 논문 작성 지원 및 시각화 데이터 생성
- **목표:** SCI 논문 게재를 위한 고품질 Figure 데이터(Raw Data/Plot) 생성 및 정량적/정성적 분석 자료 확보.

## 5. 소요 시간 및 자원 예측 (Estimation)

### 5.1. 데이터 생성 (OpenSees)
- **단일 샘플 (4방향):** 약 **40 ~ 60초** 소요 (단일 코어 기준)
    - *구성:* 모델링 + 중력해석 + 고유치해석 + 푸쉬오버해석(4회)
- **100개 샘플:** 약 **1.5 ~ 2.0 시간**
- **1,000개 샘플:** 약 **15 ~ 20 시간**
    - *비고:* `multiprocessing`을 도입하여 병렬 처리 시 코어 수(N)만큼 시간 단축 가능 (예: 8코어 사용 시 1,000개 약 2~3시간).

### 5.2. GNN 모델 학습 (Training)
- **100개 샘플 (400 그래프):** 약 **5 ~ 10분** (200 Epochs)
- **1,000개 샘플 (4,000 그래프):** 약 **30 ~ 50분**
- *비고:* 데이터 생성에 비해 학습 시간은 매우 짧음. 따라서 데이터의 **질(Quality)과 양(Quantity)** 확보가 프로젝트의 승패를 좌우함.
